{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moWJuYhYyKfT"
      },
      "source": [
        "**Importing dependencies**\n",
        "\n",
        "\n",
        "*   pip install fer\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "pckKJkWZYAiP",
        "outputId": "1d8eef67-d016-4208-896a-06cbecd88cce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: fer in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (22.4.0)\n",
            "Requirement already satisfied: keras>=2.0.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from fer) (2.9.0)\n",
            "Requirement already satisfied: tqdm in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from fer) (4.64.0)\n",
            "Requirement already satisfied: requests in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from fer) (2.28.1)\n",
            "Requirement already satisfied: opencv-contrib-python in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from fer) (4.6.0.66)\n",
            "Requirement already satisfied: mtcnn>=0.1.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from fer) (0.1.1)\n",
            "Requirement already satisfied: matplotlib in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from fer) (3.5.2)\n",
            "Requirement already satisfied: opencv-python>=4.1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from mtcnn>=0.1.1->fer) (4.6.0.66)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->fer) (1.23.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->fer) (2.8.2)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->fer) (4.34.4)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->fer) (1.4.3)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->fer) (21.3)\n",
            "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->fer) (3.0.9)\n",
            "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->fer) (9.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->fer) (0.11.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->fer) (3.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->fer) (1.26.9)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->fer) (2.1.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->fer) (2022.6.15)\n",
            "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm->fer) (0.4.4)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->fer) (1.16.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "#pip install fer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JNOgmPKdyJmd"
      },
      "outputs": [],
      "source": [
        "from fer import Video\n",
        "from fer import FER\n",
        "import os\n",
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hZ42RJV_yY0b"
      },
      "outputs": [],
      "source": [
        "location_videofile = \"videoplayback.mp4\"\n",
        "cascPath = \"abc.xml\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yapC1EpYIPpt"
      },
      "outputs": [],
      "source": [
        "def get_labels():\n",
        "      return {\n",
        "            0: \"angry\",\n",
        "            1: \"disgust\",\n",
        "            2: \"fear\",\n",
        "            3: \"happy\",\n",
        "            4: \"sad\",\n",
        "            5: \"surprise\",\n",
        "            6: \"neutral\",\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5DoVUC_QIgLt"
      },
      "outputs": [],
      "source": [
        "def tosquare(bbox):\n",
        "        \"\"\"Convert bounding box to square by elongating shorter side.\"\"\"\n",
        "        x, y, w, h = bbox\n",
        "        if h > w:\n",
        "            diff = h - w\n",
        "            x -= diff // 2\n",
        "            w += diff\n",
        "        elif w > h:\n",
        "            diff = w - h\n",
        "            y -= diff // 2\n",
        "            h += diff\n",
        "        if w != h:\n",
        "            print(f\"{w} is not {h}\")\n",
        "\n",
        "        return (x, y, w, h)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3vC6-FNtKhUu"
      },
      "outputs": [],
      "source": [
        "def apply_offsets(face_coordinates):\n",
        "      x, y, width, height = face_coordinates\n",
        "      x_off, y_off = (10, 10)\n",
        "      return (x - x_off, x + width + x_off, y - y_off, y + height + y_off)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PjlgEngjTkU6"
      },
      "outputs": [],
      "source": [
        "def preprocess_input(x, v2=False):\n",
        "        x = x.astype(\"float32\")\n",
        "        x = x / 255.0\n",
        "        if v2:\n",
        "            x = x - 0.5\n",
        "            x = x * 2.0\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kw5beMFOUKXx"
      },
      "outputs": [],
      "source": [
        "def pad(image):\n",
        "        PADDING = 40\n",
        "        row, col = image.shape[:2]\n",
        "        bottom = image[row - 2 : row, 0:col]\n",
        "        mean = cv2.mean(bottom)[0]\n",
        "\n",
        "        padded_image = cv2.copyMakeBorder(\n",
        "            image,\n",
        "            top = PADDING,\n",
        "            bottom = PADDING,\n",
        "            left = PADDING,\n",
        "            right= PADDING,\n",
        "            borderType=cv2.BORDER_CONSTANT,\n",
        "            value=[mean, mean, mean],\n",
        "        )\n",
        "        return padded_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a2XhbRSc0YvY",
        "outputId": "8fe9b2e0-321a-489e-b75f-f4ab105b9930"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Frame Count:  2195.0\n"
          ]
        }
      ],
      "source": [
        "vidcap = cv2.VideoCapture(location_videofile)\n",
        "\n",
        "success,image = vidcap.read()\n",
        "frame_count = vidcap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
        "print(\"Frame Count: \", frame_count)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yJ9ByIX70YvY"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1AIUEWhDSP4p"
      },
      "outputs": [],
      "source": [
        "from collections import OrderedDict\n",
        "from matplotlib import pyplot as plt\n",
        "import cv2\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "def predict(y):\n",
        "    Filepath = \"\"\n",
        "    vd = y\n",
        "    PADDING = 40\n",
        "    NumberofFrames = 50\n",
        "    emotion_labels = get_labels()\n",
        "    arry = {}\n",
        "\n",
        "    vidcap = cv2.VideoCapture(vd)\n",
        "\n",
        "    success,image = vidcap.read()\n",
        "    frame_count = vidcap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
        "    print(\"Frame Count: \", frame_count)\n",
        "    count = 0\n",
        "    cascPath= Filepath+\"/abc.xml\"\n",
        "    faceCascade = cv2.CascadeClassifier(cascPath)\n",
        "\n",
        "    while vidcap.isOpened():\n",
        "        score = 0\n",
        "        success,image = vidcap.read()\n",
        "\n",
        "        if success:\n",
        "            if frame_count > NumberofFrames+1:\n",
        "                count += frame_count/(NumberofFrames+1) # i.e. at 30 fps, this advances one second\n",
        "            else:\n",
        "                count += 1\n",
        "                vidcap.set(cv2.CAP_PROP_POS_FRAMES, count)\n",
        "                gray_image_array = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "                faces = faceCascade.detectMultiScale(\n",
        "                gray_image_array,\n",
        "                scaleFactor=1.1,\n",
        "                minNeighbors=5,\n",
        "                minSize=(30, 30))\n",
        "\n",
        "                if len(faces) == 1:\n",
        "                    gray_img = pad(gray_image_array)\n",
        "\n",
        "                emotions = []\n",
        "                for face_coordinates in faces:\n",
        "                    face_coordinates = tosquare(face_coordinates)\n",
        "                    x1, x2, y1, y2 = apply_offsets(face_coordinates)\n",
        "\n",
        "                    # adjust for padding\n",
        "                    x1 += PADDING\n",
        "                    x2 += PADDING\n",
        "                    y1 += PADDING\n",
        "                    y2 += PADDING\n",
        "                    x1 = np.clip(x1, a_min=0, a_max=None)\n",
        "                    y1 = np.clip(y1, a_min=0, a_max=None)\n",
        "\n",
        "                    #gray_face = gray_img[max(0, y1 - PADDING):y2 + PADDING,\n",
        "                    #                    max(0, x1 - PADDING):x2 + PADDING]\n",
        "                    #gray_face = gray_img[y1:y2, x1:x2]\n",
        "\n",
        "                    emotion_model = Filepath+\"/model1.hdf5\"\n",
        "                    model = load_model(emotion_model, compile=compile)\n",
        "                    model.make_predict_function()\n",
        "\n",
        "                    try:\n",
        "                        gray_face = cv2.resize(gray_img, model.input_shape[1:3])\n",
        "                    except Exception as e:\n",
        "                        print(\"Cannot resize \"+str(e))\n",
        "                        continue\n",
        "\n",
        "                    # Local Keras model\n",
        "                    #gray_face = preprocess_input(gray_face, True)\n",
        "                    gray_face = np.expand_dims(np.expand_dims(gray_face, 0), -1)\n",
        "\n",
        "                    emotion_prediction = model.predict(gray_face)[0]\n",
        "                    labelled_emotions = {\n",
        "                        emotion_labels[idx]: round(float(score), 2)\n",
        "                        for idx, score in enumerate(emotion_prediction)\n",
        "                    }\n",
        "\n",
        "                    emotions.append(\n",
        "                        dict(box=face_coordinates, emotions=labelled_emotions)\n",
        "                    )\n",
        "                top_emotions  = [max(e[\"emotions\"], key=lambda key: e[\"emotions\"][key]) for e in emotions]\n",
        "                if len(top_emotions):\n",
        "                    for top_emotion in emotions[0][\"emotions\"]:\n",
        "                        if top_emotion in arry.keys():\n",
        "                            arry.update({top_emotion: arry[top_emotion] + emotions[0][\"emotions\"][top_emotion]})\n",
        "                        else:\n",
        "                            arry[top_emotion] = score\n",
        "\n",
        "        else:\n",
        "            vidcap.release()\n",
        "            break\n",
        "    if len(arry) == 0:\n",
        "        return \"neutral\"\n",
        "    else:\n",
        "        return max(arry, key=arry.get)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "id": "JxJysu3oKueK",
        "outputId": "a216d226-0779-4801-c3e6-f3579072f637"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Frame Count:  2195.0\n",
            "neutral\n"
          ]
        }
      ],
      "source": [
        "emo = predict(location_videofile)\n",
        "print(emo)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4kHeB2At0Yva"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5UyE-uv0Yva"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "emotionDetection.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}